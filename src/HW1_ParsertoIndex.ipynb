{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "base64.encodestring = base64.encodebytes\n",
    "base64.decodestring = base64.decodebytes\n",
    "from pscript import py2js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install elasticsearch==7.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\", timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection\")\n",
    "files = os.listdir(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection\")\n",
    "files.remove(\"readme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get DOCNO keys for dataframe from ap89 files\n",
    "\n",
    "def get_docnums():\n",
    "    test_keys = []\n",
    "    for i in files[:364]:\n",
    "        current = open(str(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection/\"+i), encoding = \"latin-1\")\n",
    "        text = current.read()\n",
    "        keys = re.findall(\"<DOCNO>(.*)</DOCNO>\", text)\n",
    "        for j in keys:\n",
    "            test_keys.append(j)\n",
    "    return test_keys\n",
    "\n",
    "#test_keys = get_docnums()\n",
    "#print(len(test_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to merge multiple TEXTs of the same DOCNO\n",
    "\n",
    "def find_anom(text):\n",
    "    between = re.findall(\"</TEXT>(.*?)<TEXT>\", text)\n",
    "    anoms = [x for x in between if \"<DOCNO>\" not in x]\n",
    "    for n in anoms:\n",
    "        if n in text:\n",
    "            text = text.replace(n, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove information not needed in parsing\n",
    "\n",
    "def remove_others(text):\n",
    "    fileid = re.findall(\"<FILEID>(.+?)</FILEID>\", text)\n",
    "    first = re.findall(\"<FIRST>(.+?)</FIRST>\", text)\n",
    "    second = re.findall(\"<SECOND>(.+?)</SECOND>\", text)\n",
    "    head = re.findall(\"<HEAD>(.+?)</HEAD>\", text)\n",
    "    byline = re.findall(\"<BYLINE>(.+?)</BYLINE>\", text)\n",
    "    dateline = re.findall(\"<DATELINE>(.+?)</DATELINE>\", text)\n",
    "    note = re.findall(\"<NOTE>(.+?)</NOTE>\", text)\n",
    "    removal = fileid + first + second + head + byline + dateline + note\n",
    "    for x in removal:\n",
    "        if x in text:\n",
    "            text = text.replace(x, \"\")\n",
    "    sections = [\"<DOC>\", \"</DOC>\", \"<FILEID>\", \"</FILEID>\", \"<FIRST>\", \"</FIRST>\", \"<SECOND>\", \"</SECOND>\", \n",
    "    \"<HEAD>\", \"</HEAD>\", \"<BYLINE>\", \"</BYLINE>\", \"<DATELINE>\", \"</DATELINE>\", \"<NOTE>\", \"</NOTE>\", \n",
    "    \"</TEXT><TEXT>\", \"</TEXT> <TEXT>\", \"</TEXT>  <TEXT>\", \"</TEXT>   <TEXT>\", \n",
    "    \"</TEXT> files a0511-a0514-a0517-a0518; inserts new 5thgraf, In another, with leading indicators report; edits pvs 5th graf fortransition<TEXT>\", \n",
    "    \"</TEXT><UNK>Eds: For release 11 a.m. EDT, time set by source.</UNK><TEXT>\", \"</TEXT> For release 8:01 p.m. EDT<TEXT>\", \n",
    "    \"</TEXT> ^Consumer Advocates, Insurers Join Forces<TEXT>\", \"</TEXT>By s<TEXT>\", \"</TEXT>s<TEXT>\"]\n",
    "    for y in sections:\n",
    "        if y in text:\n",
    "            text = text.replace(y, \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get TEXT values for data frame\n",
    "def get_texts():\n",
    "    test_values = []\n",
    "    for i in files[:364]:\n",
    "        current = open(str(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection/\"+i), encoding = 'latin-1')\n",
    "        text = current.read()\n",
    "        text = ''.join(text.splitlines())\n",
    "        text = find_anom(text)\n",
    "        text = remove_others(text)\n",
    "        values = re.findall(\"<TEXT>(.*?)</TEXT>\", text)\n",
    "        for j in values:\n",
    "            test_values.append(j)\n",
    "    return test_values\n",
    "\n",
    "#test_values = get_texts()\n",
    "#print(len(test_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get stopwords from txt file\n",
    "\n",
    "def get_sw():\n",
    "    with open(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/stoplist.txt\") as sw:\n",
    "        stop_words = sw.read().split()\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format TEXT to remove punctuation,remove stop words, and make lowercase for \n",
    "\n",
    "def format_text():\n",
    "    formatted_values = []\n",
    "    stop_words = get_sw()\n",
    "    test_values = get_texts()\n",
    "\n",
    "    for text in test_values:\n",
    "        new_string = re.sub(\"-\", \" \", text)\n",
    "        new_string = re.sub(\"\\n\", \" \", new_string)\n",
    "        new_string = new_string.translate(str.maketrans('', '', string.punctuation))\n",
    "        new_string = ' '.join([word for word in new_string.split() if word not in stop_words])\n",
    "        formatted_values.append(new_string.lower())\n",
    "    return formatted_values\n",
    "\n",
    "#formatted_values = format_text()\n",
    "#print(formatted_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from <DOCNO> key list and <TEXT> value list\n",
    "def create_docno_text_df():\n",
    "    test_keys = get_docnums()\n",
    "    formatted_values = format_text()\n",
    "    \n",
    "    ap89_data = pd.DataFrame()\n",
    "    ap89_data[\"DOCNO\"] = test_keys\n",
    "    ap89_data[\"TEXT\"] = formatted_values\n",
    "    return ap89_data\n",
    "\n",
    "#ap89_data = create_docno_text_df()\n",
    "#print(ap89_data.iloc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create elastic search index\n",
    "def create_index():\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"TEXT\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"store\": True,\n",
    "                    \"term_vector\": \"with_positions_offsets_payloads\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                    }\n",
    "                 }\n",
    "            }\n",
    "        }\n",
    "    es.indices.create(index=\"ap89_index4\", body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write dataframe DOCNO and TEXT to elastic search index\n",
    "def write_index():\n",
    "    ap89_data = create_docno_text_df()\n",
    "    create_index()\n",
    "\n",
    "    for i, row in ap89_data.iterrows():\n",
    "        doc_num = row[\"DOCNO\"]\n",
    "        doc = {\n",
    "            \"TEXT\": row[\"TEXT\"]\n",
    "        }  \n",
    "        es.index(index=\"ap89_index4\", id=doc_num, body=doc)\n",
    "\n",
    "write_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': '1675637564', 'timestamp': '22:52:44', 'count': '84678'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use to check index\n",
    "\n",
    "es.indices.refresh(index=\"ap89_index4\")\n",
    "es.cat.count(index=\"ap89_index\", format=\"json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a5930b3c376edc2c2d456532185f29892e78707b529c32ee61aac2c2309a60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
