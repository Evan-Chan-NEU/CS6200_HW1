{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "from elasticsearch_dsl import Search\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\", timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA\")\n",
    "files = os.listdir(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection\")\n",
    "files.remove(\"readme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function (also used in parser/indexer) to get list of docnums\n",
    "def get_docnums():\n",
    "    test_keys = []\n",
    "    for i in files[:364]:\n",
    "        current = open(str(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection/\"+i), encoding = \"latin-1\")\n",
    "        text = current.read()\n",
    "        keys = re.findall(\"<DOCNO>(.*)</DOCNO>\", text)\n",
    "        for j in keys:\n",
    "            test_keys.append(j)\n",
    "    return test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_anom(text):\n",
    "    between = re.findall(\"</TEXT>(.*?)<TEXT>\", text)\n",
    "    anoms = [x for x in between if \"<DOCNO>\" not in x]\n",
    "    for n in anoms:\n",
    "        if n in text:\n",
    "            text = text.replace(n, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_others(text):\n",
    "    fileid = re.findall(\"<FILEID>(.+?)</FILEID>\", text)\n",
    "    first = re.findall(\"<FIRST>(.+?)</FIRST>\", text)\n",
    "    second = re.findall(\"<SECOND>(.+?)</SECOND>\", text)\n",
    "    head = re.findall(\"<HEAD>(.+?)</HEAD>\", text)\n",
    "    byline = re.findall(\"<BYLINE>(.+?)</BYLINE>\", text)\n",
    "    dateline = re.findall(\"<DATELINE>(.+?)</DATELINE>\", text)\n",
    "    note = re.findall(\"<NOTE>(.+?)</NOTE>\", text)\n",
    "    removal = fileid + first + second + head + byline + dateline + note\n",
    "    for x in removal:\n",
    "        if x in text:\n",
    "            text = text.replace(x, \"\")\n",
    "    sections = [\"<DOC>\", \"</DOC>\", \"<FILEID>\", \"</FILEID>\", \"<FIRST>\", \"</FIRST>\", \"<SECOND>\", \"</SECOND>\", \n",
    "    \"<HEAD>\", \"</HEAD>\", \"<BYLINE>\", \"</BYLINE>\", \"<DATELINE>\", \"</DATELINE>\", \"<NOTE>\", \"</NOTE>\", \n",
    "    \"</TEXT><TEXT>\", \"</TEXT> <TEXT>\", \"</TEXT>  <TEXT>\", \"</TEXT>   <TEXT>\", \n",
    "    \"</TEXT> files a0511-a0514-a0517-a0518; inserts new 5thgraf, In another, with leading indicators report; edits pvs 5th graf fortransition<TEXT>\", \n",
    "    \"</TEXT><UNK>Eds: For release 11 a.m. EDT, time set by source.</UNK><TEXT>\", \"</TEXT> For release 8:01 p.m. EDT<TEXT>\", \n",
    "    \"</TEXT> ^Consumer Advocates, Insurers Join Forces<TEXT>\", \"</TEXT>By s<TEXT>\", \"</TEXT>s<TEXT>\"]\n",
    "    for y in sections:\n",
    "        if y in text:\n",
    "            text = text.replace(y, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts():\n",
    "    test_values = []\n",
    "    for i in files[:364]:\n",
    "        current = open(str(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection/\"+i), encoding = 'latin-1')\n",
    "        text = current.read()\n",
    "        text = ''.join(text.splitlines())\n",
    "        text = find_anom(text)\n",
    "        text = remove_others(text)\n",
    "        values = re.findall(\"<TEXT>(.*?)</TEXT>\", text)\n",
    "        for j in values:\n",
    "            test_values.append(j)\n",
    "    return test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sw():\n",
    "    with open(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/stoplist.txt\") as sw:\n",
    "        stop_words = sw.read().split()\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text():\n",
    "    formatted_values = []\n",
    "    stop_words = get_sw()\n",
    "    test_values = get_texts()\n",
    "\n",
    "    for text in test_values:\n",
    "        new_string = re.sub(\"-\", \" \", text)\n",
    "        new_string = re.sub(\"\\n\", \" \", new_string)\n",
    "        new_string = new_string.translate(str.maketrans('', '', string.punctuation))\n",
    "        new_string = ' '.join([word for word in new_string.split() if word not in stop_words])\n",
    "        formatted_values.append(new_string.lower())\n",
    "    return formatted_values\n",
    "\n",
    "#formatted_values = format_text()\n",
    "#print(formatted_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get vocabulary size of corpus - unique terms in collection\n",
    "def get_vocab_size():\n",
    "    formatted_values = format_text()\n",
    "    one_string = \" \".join(formatted_values)\n",
    "    all_words = one_string.split()\n",
    "    unique_words = set(all_words)\n",
    "    vocab = len(unique_words)\n",
    "    return vocab\n",
    "\n",
    "#get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for get_queries() to return the number of each query\n",
    "def get_queries_nums(queries_list):\n",
    "    queries_nums = []\n",
    "    for line in queries_list:\n",
    "        num = line.split()[0]\n",
    "        queries_nums.append(num)\n",
    "    return queries_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for get_queries() to return necessary text of each query with\n",
    "def get_queries_text(queries_list):\n",
    "    queries_nums = get_queries_nums(queries_list)\n",
    "    query_stop_words = [\"Document\", \"will\", \"must\", \"discuss\", \n",
    "    \"report\", \"include\", \"describe\", \"identify\", \"a\", \"an\", \"as\",\n",
    "    \"and\", \"the\", \"to\", \"or\", \"either\", \"of\", \"by\", \"in\", \"with\", \n",
    "    \"about\", \"some\", \"any\", \"its\", \"even\", \"other\", \"which\",\n",
    "    \"being\", \"certain\", \"has\"]\n",
    "    remove_words = queries_nums + query_stop_words\n",
    "    queries_text = []\n",
    "    for i in queries_list:\n",
    "        x = i.split()\n",
    "        queries_text.append(\" \".join(a if a not in remove_words else '' for a in x))\n",
    "    return queries_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get dataframe with query number and query text\n",
    "def get_queries_df():\n",
    "    queries_df = pd.DataFrame()\n",
    "    with open(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/query_desc.51-100.short.txt\") as queries_file:\n",
    "        queries_list = queries_file.read()\n",
    "        queries_list = re.sub(\"-\", \" \", queries_list)\n",
    "        queries_list = queries_list.translate(str.maketrans('', '', string.punctuation))\n",
    "        queries_list = queries_list.split(\"\\n\")\n",
    "        queries_nums = get_queries_nums(queries_list)\n",
    "        queries_text = get_queries_text(queries_list)\n",
    "        queries_df[\"QueryNumber\"] = queries_nums\n",
    "        queries_df[\"QueryText\"] = queries_text\n",
    "    return queries_df\n",
    "\n",
    "#queries_df = get_queries_df()\n",
    "#print(queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get list of queries \n",
    "def get_queries_words(queries_df):\n",
    "    query_words_list = []\n",
    "    for query in queries_df.iloc[:, 1]:\n",
    "        query_words_list.append(query)\n",
    "    return query_words_list\n",
    "\n",
    "#queries_df = get_queries_df()\n",
    "#query_words_list = get_queries_words(queries_df)\n",
    "#print(query_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to search term vector api result for query word and return its frequency within a document - how many times it is in document\n",
    "def get_tf_num(term, results):\n",
    "    tf_num = 0\n",
    "    if results.__contains__(\"TEXT\") is True:\n",
    "        if term in results[\"TEXT\"][\"terms\"]:\n",
    "            tf = results[\"TEXT\"][\"terms\"][term][\"term_freq\"]\n",
    "            tf_num += tf\n",
    "            return tf_num\n",
    "        else:\n",
    "            return tf_num\n",
    "    else:\n",
    "        return tf_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to search term vector api result for query word and return its document frequency - how many documents it is in\n",
    "def get_df_num(term, results):\n",
    "    df_num = 0\n",
    "    if results.__contains__(\"TEXT\") is True:\n",
    "        if term in results[\"TEXT\"][\"terms\"]:\n",
    "            df = results[\"TEXT\"][\"terms\"][term][\"doc_freq\"]\n",
    "            df_num += df\n",
    "            return df_num\n",
    "        else:\n",
    "            return df_num\n",
    "    else:\n",
    "        return df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get term frequency of query word within the query - how many times is it in the query\n",
    "def get_tq_num(term, query):\n",
    "    tq_num = query.count(term)\n",
    "    return tq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to retrieve dictionary of query words tf within a document\n",
    "def get_docnum_tf_dict(docnum, query):\n",
    "    q_word_dict = {}\n",
    "    results = es.termvectors(index=\"ap89_index4\",\n",
    "                        id=str(docnum),\n",
    "                        body={\n",
    "                            \"fields\": [\"TEXT\"],\n",
    "                            \"term_statistics\": True,\n",
    "                            \"field_statistics\": True\n",
    "                        })[\"term_vectors\"]\n",
    "    for term in query:\n",
    "        q_word_dict[term] = [get_tf_num(term, results), get_df_num(term, results), get_tq_num(term, query)]\n",
    "    \n",
    "    return q_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve term frequency values and document frequency values of words in all 25 queries in each of the 84678 documents\n",
    "# returns dictionary of {docnum : {q_word : [tf, df, tq], q_word : [tf, df, tq]}, docnum : {q_word : [tf, df, tq], q_word : [tf, df, tq]}, etc.}\n",
    "def create_tf_df_tq_dict():\n",
    "    queries_df = get_queries_df()\n",
    "    q_nums_list = queries_df.iloc[:, 0]\n",
    "    q_words_list = queries_df.iloc[:, 1]\n",
    "    test_keys = get_docnums()\n",
    "\n",
    "    tf_df_tq_dict_list = []\n",
    "    for list in q_words_list[:25]:\n",
    "        query = list.split()\n",
    "        tf_dict = {}\n",
    "        for docnum in test_keys[:84678]:\n",
    "            tf_dict[docnum] = get_docnum_tf_dict(docnum, query)\n",
    "        tf_df_tq_dict_list.append(tf_dict)\n",
    "    \n",
    "    return tf_df_tq_dict_list\n",
    "\n",
    "#tf_df_tq_dict_list = create_tf_df_tq_dict()\n",
    "#print(tf_df_tq_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get the doc length of a given doc\n",
    "def get_doc_length(doc):\n",
    "    doc_length = 0\n",
    "   \n",
    "    results = es.termvectors(index=\"ap89_index4\",\n",
    "                        id=str(doc),\n",
    "                        body={\n",
    "                            \"fields\": [\"TEXT\"],\n",
    "                            \"term_statistics\": True,\n",
    "                            \"field_statistics\": True\n",
    "                        })[\"term_vectors\"]\n",
    "\n",
    "    if results.__contains__(\"TEXT\") is True:\n",
    "        for term in results[\"TEXT\"][\"terms\"]:\n",
    "            tf_val = 1 * (results[\"TEXT\"][\"terms\"][term][\"term_freq\"])\n",
    "            doc_length += tf_val\n",
    "        return doc_length\n",
    "    else:\n",
    "        return doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get dataframe of uni lap scores for 1 query for all documents\n",
    "def get_uni_lap_df(tf_df_tq_dict, count):\n",
    " \n",
    "   doc_length_list = []\n",
    "   for doc in tf_df_tq_dict.keys():\n",
    "      doc_length = get_doc_length(doc)\n",
    "      doc_length_list.append(doc_length)\n",
    "   \n",
    "   doc_id_list = []\n",
    "   for doc in tf_df_tq_dict.keys():\n",
    "      doc_id_list.append(doc)\n",
    "\n",
    "   test_keys = get_docnums()\n",
    "   corpus_size = len(test_keys)\n",
    "   avg_doc_length = (sum(doc_length_list))/corpus_size\n",
    "\n",
    "   queries_df = get_queries_df()\n",
    "   q_n_list = queries_df[\"QueryNumber\"].tolist()\n",
    "\n",
    "   uni_lap_score = []\n",
    "   q_tf_df_tq_list = [i for i in tf_df_tq_dict.values()]\n",
    "   vocab = get_vocab_size()\n",
    "   for index, list in enumerate(q_tf_df_tq_list):\n",
    "      current_list = []\n",
    "      uni_lap = 0\n",
    "      for tf_df_tq in list.values():\n",
    "         doc_length = doc_length_list[index]\n",
    "         \n",
    "         numer = (tf_df_tq[0] + 1)\n",
    "         denom = (doc_length + vocab)\n",
    "         \n",
    "         if tf_df_tq[0] == 0:\n",
    "            uni_lap += -1000\n",
    "         else:\n",
    "            uni_lap += math.log(numer/denom)\n",
    "         \n",
    "         current_list.append(uni_lap)\n",
    "\n",
    "      uni_lap_score.append(sum(current_list))\n",
    "   \n",
    "   uni_lap_df = pd.DataFrame()\n",
    "   uni_lap_df = uni_lap_df.reset_index()\n",
    "   uni_lap_df[\"QUERYNUM\"] = [q_n_list[count]] * len(doc_id_list)\n",
    "   uni_lap_df[\"DOCNO\"] = doc_id_list\n",
    "   uni_lap_df[\"SCORE\"] = uni_lap_score\n",
    "   result_df = uni_lap_df.sort_values(by=\"SCORE\", ascending=False)\n",
    "   result_df = result_df.iloc[:1000]\n",
    "\n",
    "   return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to write retrieval model results to output file\n",
    "def unigram_laplace():\n",
    "    tf_df_tq_dict_list = create_tf_df_tq_dict()\n",
    "\n",
    "    uni_lap_results_list = []\n",
    "    count = 0\n",
    "    for tf_df_tq_dict in tf_df_tq_dict_list:\n",
    "        result_df = get_uni_lap_df(tf_df_tq_dict, count)\n",
    "        count += 1\n",
    "        uni_lap_results_list.append(result_df)\n",
    "    \n",
    "    for df in uni_lap_results_list:\n",
    "        query_number_result = df[\"QUERYNUM\"].tolist()\n",
    "        query_result_docnums = df[\"DOCNO\"].tolist()\n",
    "        query_result_scores = df[\"SCORE\"].tolist()\n",
    "        with open('/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/Unigram_LM_Laplace_smoothing_results.txt', 'a') as queryResults:\n",
    "            rank = 1\n",
    "            j = 0\n",
    "            while j in range(len(query_result_docnums)):\n",
    "                queryResults.write('%s Q0 %s %s %s Exp\\n' % (query_number_result[j], query_result_docnums[j], rank, query_result_scores[j]))\n",
    "                rank += 1\n",
    "                j += 1\n",
    "        queryResults.close()\n",
    "\n",
    "unigram_laplace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a5930b3c376edc2c2d456532185f29892e78707b529c32ee61aac2c2309a60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
