{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "from elasticsearch_dsl import Search\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\", timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA\")\n",
    "files = os.listdir(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection\")\n",
    "files.remove(\"readme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function (also used in parser/indexer) to get list of docnums\n",
    "def get_docnums():\n",
    "    test_keys = []\n",
    "    for i in files[:364]:\n",
    "        current = open(str(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/ap89_collection/\"+i), encoding = \"latin-1\")\n",
    "        text = current.read()\n",
    "        keys = re.findall(\"<DOCNO>(.*)</DOCNO>\", text)\n",
    "        for j in keys:\n",
    "            test_keys.append(j)\n",
    "    return test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for get_queries() to return the number of each query\n",
    "def get_queries_nums(queries_list):\n",
    "    queries_nums = []\n",
    "    for line in queries_list:\n",
    "        num = line.split()[0]\n",
    "        queries_nums.append(num)\n",
    "    return queries_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for get_queries() to return necessary text of each query with\n",
    "def get_queries_text(queries_list):\n",
    "    queries_nums = get_queries_nums(queries_list)\n",
    "    query_stop_words = [\"Document\", \"will\", \"must\", \"discuss\", \n",
    "    \"report\", \"include\", \"describe\", \"identify\", \"a\", \"an\", \"as\",\n",
    "    \"and\", \"the\", \"to\", \"or\", \"either\", \"of\", \"by\", \"in\", \"with\", \n",
    "    \"about\", \"some\", \"any\", \"its\", \"even\", \"other\", \"which\",\n",
    "    \"being\", \"certain\", \"has\"]\n",
    "    remove_words = queries_nums + query_stop_words\n",
    "    queries_text = []\n",
    "    for i in queries_list:\n",
    "        x = i.split()\n",
    "        queries_text.append(\" \".join(a if a not in remove_words else '' for a in x))\n",
    "    return queries_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get dataframe with query number and query text\n",
    "def get_queries_df():\n",
    "    queries_df = pd.DataFrame()\n",
    "    with open(\"/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/IR_data/AP_DATA/query_desc.51-100.short.txt\") as queries_file:\n",
    "        queries_list = queries_file.read()\n",
    "        queries_list = re.sub(\"-\", \" \", queries_list)\n",
    "        queries_list = queries_list.translate(str.maketrans('', '', string.punctuation))\n",
    "        queries_list = queries_list.split(\"\\n\")\n",
    "        queries_nums = get_queries_nums(queries_list)\n",
    "        queries_text = get_queries_text(queries_list)\n",
    "        queries_df[\"QueryNumber\"] = queries_nums\n",
    "        queries_df[\"QueryText\"] = queries_text\n",
    "    return queries_df\n",
    "\n",
    "#queries_df = get_queries_df()\n",
    "#print(queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get list of queries \n",
    "def get_queries_words(queries_df):\n",
    "    query_words_list = []\n",
    "    for query in queries_df.iloc[:, 1]:\n",
    "        query_words_list.append(query)\n",
    "    return query_words_list\n",
    "\n",
    "#queries_df = get_queries_df()\n",
    "#query_words_list = get_queries_words(queries_df)\n",
    "#print(query_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to search term vector api result for query word and return its frequency within a document\n",
    "def get_tf_num(term, results):\n",
    "    tf_num = 0\n",
    "    if results.__contains__(\"TEXT\") is True:\n",
    "        if term in results[\"TEXT\"][\"terms\"]:\n",
    "            tf = results[\"TEXT\"][\"terms\"][term][\"term_freq\"]\n",
    "            tf_num += tf\n",
    "            return tf_num\n",
    "        else:\n",
    "            return tf_num\n",
    "    else:\n",
    "        return tf_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to retrieve dictionary of query words tf within a document\n",
    "def get_docnum_tf_dict(docnum, query):\n",
    "    q_word_dict = {}\n",
    "    results = es.termvectors(index=\"ap89_index4\",\n",
    "                        id=str(docnum),\n",
    "                        body={\n",
    "                            \"fields\": [\"TEXT\"],\n",
    "                            \"term_statistics\": True,\n",
    "                            \"field_statistics\": True\n",
    "                        })[\"term_vectors\"]\n",
    "    for term in query:\n",
    "        q_word_dict[term] = get_tf_num(term, results)\n",
    "    \n",
    "    return q_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve term frequency values of words in all 25 queries in each of the 84678 documents\n",
    "# returns dictionary of {docnum : {q_word : tf, q_word : tf}, docnum : {q_word : tf, q_word : tf}, etc.}\n",
    "def create_tf_dict():\n",
    "    queries_df = get_queries_df()\n",
    "    q_nums_list = queries_df.iloc[:, 0]\n",
    "    q_words_list = queries_df.iloc[:, 1]\n",
    "    test_keys = get_docnums()\n",
    "\n",
    "    tf_dict_list = []\n",
    "    for list in q_words_list[:25]:          \n",
    "        query = list.split()\n",
    "        tf_dict = {}\n",
    "        for docnum in test_keys[:84678]:\n",
    "            tf_dict[docnum] = get_docnum_tf_dict(docnum, query)\n",
    "        tf_dict_list.append(tf_dict)\n",
    "    \n",
    "    return tf_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get the doc length of a given doc\n",
    "def get_doc_length(doc):\n",
    "    doc_length = 0\n",
    "   \n",
    "    results = es.termvectors(index=\"ap89_index4\",\n",
    "                        id=str(doc),\n",
    "                        body={\n",
    "                            \"fields\": [\"TEXT\"],\n",
    "                            \"term_statistics\": True,\n",
    "                            \"field_statistics\": True\n",
    "                        })[\"term_vectors\"]\n",
    "\n",
    "    if results.__contains__(\"TEXT\") is True:\n",
    "        for term in results[\"TEXT\"][\"terms\"]:\n",
    "            tf_val = 1 * (results[\"TEXT\"][\"terms\"][term][\"term_freq\"])\n",
    "            doc_length += tf_val\n",
    "        return doc_length\n",
    "    else:\n",
    "        return doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get dataframe of okapi_tf scores for 1 query for all documents\n",
    "def get_okapi_df(tf_dict, count):\n",
    " \n",
    "   doc_length_list = []\n",
    "   for doc in tf_dict.keys():\n",
    "      doc_length = get_doc_length(doc)\n",
    "      doc_length_list.append(doc_length)\n",
    "   \n",
    "   doc_id_list = []\n",
    "   for doc in tf_dict.keys():\n",
    "      doc_id_list.append(doc)\n",
    "\n",
    "   test_keys = get_docnums()\n",
    "   corpus_size = len(test_keys)\n",
    "   avg_doc_length = (sum(doc_length_list))/corpus_size\n",
    "\n",
    "   queries_df = get_queries_df()\n",
    "   q_n_list = queries_df[\"QueryNumber\"].tolist()\n",
    "\n",
    "   okapi_tf_score = []\n",
    "   q_tf_list = [i for i in tf_dict.values()]\n",
    "   for index, list in enumerate(q_tf_list):\n",
    "      current_list = []\n",
    "      okapi_score = 0\n",
    "      for tf in list.values():\n",
    "         doc_length = doc_length_list[index]\n",
    "         d = (doc_length/avg_doc_length)\n",
    "         denom = tf + 0.5 + 1.5 * d\n",
    "         okapi_score += (tf/denom)\n",
    "         current_list.append(okapi_score)\n",
    "      okapi_tf_score.append(sum(current_list))\n",
    "   \n",
    "   okapi_df = pd.DataFrame()\n",
    "   okapi_df = okapi_df.reset_index()\n",
    "   okapi_df[\"QUERYNUM\"] = [q_n_list[count]] * len(doc_id_list)\n",
    "   okapi_df[\"DOCNO\"] = doc_id_list\n",
    "   okapi_df[\"OKAPI\"] = okapi_tf_score\n",
    "   result_df = okapi_df.sort_values(by=\"OKAPI\", ascending=False)\n",
    "   result_df = result_df.iloc[:1000]\n",
    "\n",
    "   return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_tf():\n",
    "    tf_dict_list = create_tf_dict() \n",
    "\n",
    "    okapi_results_list = []\n",
    "    count = 0\n",
    "    for tf_dict in tf_dict_list:\n",
    "        result_df = get_okapi_df(tf_dict, count)\n",
    "        count += 1\n",
    "        okapi_results_list.append(result_df)\n",
    "    \n",
    "    for df in okapi_results_list:\n",
    "        query_number_result = df[\"QUERYNUM\"].tolist()\n",
    "        query_result_docnums = df[\"DOCNO\"].tolist()\n",
    "        query_result_scores = df[\"OKAPI\"].tolist()\n",
    "        with open('/Users/Dibble/Desktop/homework-1-Evan-Chan-NEU-main/Okapi_TF_results.txt', 'a') as queryResults:\n",
    "            rank = 1\n",
    "            j = 0\n",
    "            while j in range(len(query_result_docnums)):\n",
    "                queryResults.write('%s Q0 %s %s %s Exp\\n' % (query_number_result[j], query_result_docnums[j], rank, query_result_scores[j]))\n",
    "                rank += 1\n",
    "                j += 1\n",
    "        queryResults.close()\n",
    "\n",
    "okapi_tf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a5930b3c376edc2c2d456532185f29892e78707b529c32ee61aac2c2309a60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
